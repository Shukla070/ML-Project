{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c63a49d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the pre-trained MarianMT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26627e0f27c947588271bdd4a83aef04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "source.spm:   0%|          | 0.00/768k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\huggingface_cache\\hub\\models--Helsinki-NLP--opus-mt-en-de. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4f110860f44e8bb5a6b7c0da787c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "target.spm:   0%|          | 0.00/797k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7c3086127544449cfbf31738fce574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "--- Test Translation ---\n",
      "Original English: A man is sitting on a bench.\n",
      "Model's German Translation: Ein Mann sitzt auf einer Bank.\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the pre-trained MarianMT model for EN-DE translation, as suggested by the project plan.\n",
    "# The \"pipeline\" function from Hugging Face handles the model and tokenizer loading for us.\n",
    "print(\"Loading the pre-trained MarianMT model...\")\n",
    "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-de\", device=0) # device=0 uses the first GPU\n",
    "\n",
    "# Let's do a quick test to see the model in action\n",
    "test_sentence = \"A man is sitting on a bench.\"\n",
    "translated_output = translator(test_sentence)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"\\n--- Test Translation ---\")\n",
    "print(f\"Original English: {test_sentence}\")\n",
    "print(f\"Model's German Translation: {translated_output[0]['translation_text']}\")\n",
    "print(\"------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7df6c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Multi30k test data...\n",
      "Loaded 1000 sentences from the test set.\n",
      "\n",
      "--- Sample Data ---\n",
      "Example 1 Source (EN): A man in an orange hat starring at something.\n",
      "Example 1 Reference (DE): Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "\n",
      "Example 2 Source (EN): A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "Example 2 Reference (DE): Ein Boston Terrier läuft über saftig-grünes Gras vor einem weißen Zaun.\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading the Multi30k test data...\")\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"bentrevett/multi30k\")\n",
    "\n",
    "# Extract the 'test' split\n",
    "test_data = dataset['test']\n",
    "\n",
    "# Create a list of English source sentences\n",
    "source_sentences = [example['en'] for example in test_data]\n",
    "\n",
    "# Create a list of German reference translations\n",
    "reference_translations = [example['de'] for example in test_data]\n",
    "\n",
    "print(f\"Loaded {len(source_sentences)} sentences from the test set.\")\n",
    "\n",
    "# Let's inspect the first two examples to make sure they're correct\n",
    "print(\"\\n--- Sample Data ---\")\n",
    "print(f\"Example 1 Source (EN): {source_sentences[0]}\")\n",
    "print(f\"Example 1 Reference (DE): {reference_translations[0]}\")\n",
    "print(f\"\\nExample 2 Source (EN): {source_sentences[1]}\")\n",
    "print(f\"Example 2 Reference (DE): {reference_translations[1]}\")\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94074f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations for the 1,000 test sentences...\n",
      "This may take a minute...\n",
      "\n",
      "Translation generation complete.\n",
      "Successfully generated 1000 translations.\n",
      "\n",
      "--- Sample Translations Comparison ---\n",
      "Example 1:\n",
      "  Source (EN):      A man in an orange hat starring at something.\n",
      "  Reference (DE):   Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "  Model Output (DE): Ein Mann in einem orangenen Hut, der mit etwas zu tun hat.\n",
      "--------------------\n",
      "Example 2:\n",
      "  Source (EN):      A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "  Reference (DE):   Ein Boston Terrier läuft über saftig-grünes Gras vor einem weißen Zaun.\n",
      "  Model Output (DE): Ein Boston Terrier läuft auf üppig grünem Gras vor einem weißen Zaun.\n",
      "--------------------\n",
      "Example 3:\n",
      "  Source (EN):      A girl in karate uniform breaking a stick with a front kick.\n",
      "  Reference (DE):   Ein Mädchen in einem Karateanzug bricht ein Brett mit einem Tritt.\n",
      "  Model Output (DE): Ein Mädchen in Karate-Uniform bricht einen Stock mit einem Front-Kick.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating translations for the 1,000 test sentences...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "# The 'translator' pipeline can process a list of sentences directly.\n",
    "# It will automatically use the GPU and batch the data for efficiency.\n",
    "# We'll specify a batch size for good performance.\n",
    "model_outputs = translator(source_sentences, batch_size=32)\n",
    "\n",
    "# The output is a list of dictionaries. We need to extract the actual translated text.\n",
    "model_translations = [output['translation_text'] for output in model_outputs]\n",
    "\n",
    "print(\"\\nTranslation generation complete.\")\n",
    "print(f\"Successfully generated {len(model_translations)} translations.\")\n",
    "\n",
    "# Let's look at the first 3 examples to see how the model did\n",
    "print(\"\\n--- Sample Translations Comparison ---\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Source (EN):      {source_sentences[i]}\")\n",
    "    print(f\"  Reference (DE):   {reference_translations[i]}\")\n",
    "    print(f\"  Model Output (DE): {model_translations[i]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ede3b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- BLEU Score Sanity Check ---\n",
      "The BLEU score is: 36.25\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# The list of translations generated by the pre-trained model\n",
    "hypotheses = model_translations\n",
    "\n",
    "# The list of correct translations from the dataset.\n",
    "# sacrebleu expects references to be in a list of lists, as there can be\n",
    "# multiple correct translations for a single source. We only have one.\n",
    "references = [reference_translations]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "\n",
    "print(\"--- BLEU Score Sanity Check ---\")\n",
    "print(f\"The BLEU score is: {bleu.score:.2f}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf6aba",
   "metadata": {},
   "source": [
    "EVALUATE OUR TRAINED MODEL NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b1db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizers...\n",
      "Tokenizers loaded successfully.\n",
      "Model architecture defined successfully.\n",
      "Hyperparameters defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# --- 1. Load Tokenizers ---\n",
    "print(\"Loading tokenizers...\")\n",
    "tokenizer_en = Tokenizer.from_file(\"tokenizer_en.json\")\n",
    "tokenizer_de = Tokenizer.from_file(\"tokenizer_de.json\")\n",
    "print(\"Tokenizers loaded successfully.\")\n",
    "\n",
    "# --- 2. Re-define the Model Architecture ---\n",
    "# We need to define the model's structure again so we can load the weights into it.\n",
    "# This code is the same as from your training script.\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, emb_size: int, dropout: float, maxlen: int = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        den = torch.exp(-torch.arange(0, emb_size, 2) * math.log(10000) / emb_size)\n",
    "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
    "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
    "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
    "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
    "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('pos_embedding', pos_embedding)\n",
    "\n",
    "    def forward(self, token_embedding):\n",
    "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size: int, emb_size):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.emb_size = emb_size\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        return self.embedding(tokens.long()) * math.sqrt(self.emb_size)\n",
    "\n",
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, num_encoder_layers: int, num_decoder_layers: int, emb_size: int, nhead: int,\n",
    "                 src_vocab_size: int, tgt_vocab_size: int, dim_feedforward: int = 512, dropout: float = 0.1):\n",
    "        super(Seq2SeqTransformer, self).__init__()\n",
    "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
    "                                          num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
    "                                          dropout=dropout)\n",
    "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
    "        self.src_tok_emb = TokenEmbedding(src_vocab_size, emb_size)\n",
    "        self.tgt_tok_emb = TokenEmbedding(tgt_vocab_size, emb_size)\n",
    "        self.positional_encoding = PositionalEncoding(emb_size, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n",
    "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
    "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
    "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
    "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
    "        return self.generator(outs)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.transformer.encoder(self.positional_encoding(self.src_tok_emb(src)), src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        return self.transformer.decoder(self.positional_encoding(self.tgt_tok_emb(tgt)), memory, tgt_mask)\n",
    "\n",
    "print(\"Model architecture defined successfully.\")\n",
    "\n",
    "\n",
    "# --- 3. Define Hyperparameters ---\n",
    "# These must be identical to the parameters used during training.\n",
    "# [cite_start]The model size was kept small as suggested by the project plan for the baseline model[cite: 4].\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SRC_VOCAB_SIZE = 10000\n",
    "TGT_VOCAB_SIZE = 10000\n",
    "EMB_SIZE = 256\n",
    "NHEAD = 8\n",
    "FFN_HID_DIM = 512\n",
    "NUM_ENCODER_LAYERS = 3\n",
    "NUM_DECODER_LAYERS = 3\n",
    "print(\"Hyperparameters defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f762f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading saved model weights from 'best_model.pth'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hp\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is loaded and ready for inference.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Instantiate the model with the defined hyperparameters\n",
    "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
    "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, FFN_HID_DIM)\n",
    "\n",
    "# Load the saved model weights\n",
    "print(\"Loading saved model weights from 'best_model.pth'...\")\n",
    "transformer.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Move the model to the GPU\n",
    "transformer = transformer.to(DEVICE)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "# This is important as it disables layers like dropout during inference\n",
    "transformer.eval()\n",
    "\n",
    "print(\"Model is loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ccb902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inference Test on Your Model ---\n",
      "Source (EN):      A man in an orange hat starring at something.\n",
      "Reference (DE):   Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "Your Model (DE):  Ein Mann mit einem orangefarbenen Hut arbeitet an etwas .\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Helper function for mask creation ---\n",
    "def generate_square_subsequent_mask(sz, device):\n",
    "    mask = (torch.triu(torch.ones((sz, sz), device=device)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask\n",
    "\n",
    "# --- The main translation function ---\n",
    "def translate_sentence(model, src_sentence: str, device, max_len=50):\n",
    "    model.eval()\n",
    "\n",
    "    # Define special symbols and their indices\n",
    "    UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "\n",
    "    # Tokenize the source sentence and add SOS/EOS tokens\n",
    "    src_tokens = [SOS_IDX] + tokenizer_en.encode(src_sentence).ids + [EOS_IDX]\n",
    "    src_tensor = torch.LongTensor(src_tokens).unsqueeze(1).to(device)\n",
    "    src_len = src_tensor.shape[0]\n",
    "\n",
    "    # Create the source mask\n",
    "    src_mask = torch.zeros((src_len, src_len), device=device).type(torch.bool)\n",
    "    \n",
    "    # Get the encoded sentence (memory) from the encoder\n",
    "    with torch.no_grad():\n",
    "        memory = model.encode(src_tensor, src_mask)\n",
    "    \n",
    "    # Start the decoding with the SOS token\n",
    "    ys = torch.ones(1, 1).fill_(SOS_IDX).type(torch.long).to(device)\n",
    "    \n",
    "    for i in range(max_len - 1):\n",
    "        with torch.no_grad():\n",
    "            tgt_len = ys.shape[0]\n",
    "            tgt_mask = (generate_square_subsequent_mask(tgt_len, device).type(torch.bool)).to(device)\n",
    "            \n",
    "            # Decode the next token\n",
    "            out = model.decode(ys, memory, tgt_mask)\n",
    "            out = out.transpose(0, 1)\n",
    "            \n",
    "            # Get the probability distribution and find the most likely next token\n",
    "            prob = model.generator(out[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "        # Append the new token to our sequence\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src_tensor.data).fill_(next_word)], dim=0)\n",
    "        \n",
    "        # If the model predicts the EOS token, we stop generating\n",
    "        if next_word == EOS_IDX:\n",
    "            break\n",
    "            \n",
    "    # Convert the generated token IDs back to a string\n",
    "    # We remove the SOS token at the beginning\n",
    "    tgt_tokens = ys.flatten().tolist()\n",
    "    return tokenizer_de.decode(tgt_tokens[1:])\n",
    "\n",
    "\n",
    "# --- Let's test our function on an example ---\n",
    "# (Using the same test data lists from Part B)\n",
    "test_sentence_index = 0\n",
    "source_sentence = source_sentences[test_sentence_index]\n",
    "reference_translation = reference_translations[test_sentence_index]\n",
    "\n",
    "# Generate the translation using your model\n",
    "model_translation = translate_sentence(transformer, source_sentence, DEVICE)\n",
    "\n",
    "print(\"--- Inference Test on Your Model ---\")\n",
    "print(f\"Source (EN):      {source_sentence}\")\n",
    "print(f\"Reference (DE):   {reference_translation}\")\n",
    "print(f\"Your Model (DE):  {model_translation}\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e104ad2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translations for the entire test set using your trained model...\n",
      "This will take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:42<00:00, 23.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Translation generation complete.\n",
      "\n",
      "--- Sample Translations Comparison (Your Model) ---\n",
      "Example 1:\n",
      "  Source (EN):         A man in an orange hat starring at something.\n",
      "  Reference (DE):      Ein Mann mit einem orangefarbenen Hut, der etwas anstarrt.\n",
      "  Your Model's Output: Ein Mann mit einem orangefarbenen Hut arbeitet an etwas .\n",
      "--------------------\n",
      "Example 2:\n",
      "  Source (EN):         A Boston Terrier is running on lush green grass in front of a white fence.\n",
      "  Reference (DE):      Ein Boston Terrier läuft über saftig-grünes Gras vor einem weißen Zaun.\n",
      "  Your Model's Output: Ein Surfer läuft auf einem grünen Gras vor einem weißen Zaun .\n",
      "--------------------\n",
      "Example 3:\n",
      "  Source (EN):         A girl in karate uniform breaking a stick with a front kick.\n",
      "  Reference (DE):      Ein Mädchen in einem Karateanzug bricht ein Brett mit einem Tritt.\n",
      "  Your Model's Output: Ein Mädchen in einem Trikot macht einen Stock vor einem Stock .\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "print(\"Generating translations for the entire test set using your trained model...\")\n",
    "print(\"This will take a few minutes...\")\n",
    "\n",
    "my_model_translations = []\n",
    "# Loop through each source sentence and translate it\n",
    "for sentence in tqdm(source_sentences):\n",
    "    translation = translate_sentence(transformer, sentence, DEVICE)\n",
    "    my_model_translations.append(translation)\n",
    "\n",
    "print(\"\\nTranslation generation complete.\")\n",
    "\n",
    "# --- Let's inspect the first few results from your model ---\n",
    "print(\"\\n--- Sample Translations Comparison (Your Model) ---\")\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"  Source (EN):         {source_sentences[i]}\")\n",
    "    print(f\"  Reference (DE):      {reference_translations[i]}\")\n",
    "    print(f\"  Your Model's Output: {my_model_translations[i]}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebe9d45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Evaluation of Your Baseline Model ---\n",
      "BLEU Score: 22.12\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import sacrebleu\n",
    "\n",
    "# The list of translations generated by YOUR trained model\n",
    "hypotheses = my_model_translations\n",
    "\n",
    "# The list of correct reference translations\n",
    "references = [reference_translations]\n",
    "\n",
    "# Calculate the BLEU score\n",
    "bleu = sacrebleu.corpus_bleu(hypotheses, references)\n",
    "\n",
    "print(\"--- Final Evaluation of Your Baseline Model ---\")\n",
    "print(f\"BLEU Score: {bleu.score:.2f}\")\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9470e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
